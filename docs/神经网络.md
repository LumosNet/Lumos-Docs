### 神经网络的基本流程

前向传播、反向传播、权重更新

- 前向传播

  顺序计算每一层输出阈值，最后计算出模型损失

- 反向传播

  反向传播模型损失，计算权重梯度

- 权重更新

  利用梯度下降法更新权重

任何神经网络模型都遵守该基本流程



### DNN

深度全连接神经网络的基本过程是最清晰的。由于该模型可以更好的划分为线性模型的复合，所以其推到流程十分简单。

#### 模型抽象

设有一个DNN模型有以下参数设置

- 损失函数L
- 隐层激活函数σ
- 隐层权重w
- 隐层偏移量b
- 梯度δ

我们做如下声明

隐层计算输出（没有激活）z，隐层计算输出（通过激活）a

隐层计算如下：
$$
\begin{aligned}
&z_{l}=w_{l}a_{l-1}+b_{l} \\
&a_{l}=\sigma(z_{l})=\sigma(w_{l}a_{l-1}+b_{l})
\end{aligned}
$$
设隐层计算为一个函数g，则整个DNN模型可表示为函数f
$$
f=L(g_{l}(g_{l-1}(...g(x))))
$$
可以看出整个DNN模型是一个线性函数的复合



#### 反向推导

由于我们利用梯度下降法来更新模型权重，所以我们需要求每一个权重关于损失的梯度

求梯度，我们就要求损失关于权重的偏导（梯度：全部偏导数组成的向量）
$$
\frac{\partial L}{\partial w}\quad \frac{\partial L}{\partial b}
$$
由复合函数求导的链式法则，有如下过程：

设求第k层权重w，b的梯度
$$
\begin{aligned}
&D_{w_{k}}=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \dot\ \frac{\partial z_{k}}{\partial w_{k}} \\
&D_{b_{k}}=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \dot\ \frac{\partial z_{k}}{\partial b_{k}}
\end{aligned}
$$
可以看出这是一个可递归过程，设输出层为第l（小写L）层，则输出层权重梯度为
$$
\begin{aligned}
&D_{w_{l}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial w_{l}} \\
&D_{b_{l}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial b_{l}}
\end{aligned}
$$
第l-1层权重梯度为
$$
\begin{aligned}
&D_{w_{l-1}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial w_{l-1}} \\
&D_{b_{l-1}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial b_{l-1}}
\end{aligned}
$$
第l-2层权重梯度为
$$
\begin{aligned}
&D_{w_{l-2}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial z_{l-2}} \dot\ \frac{\partial z_{l-2}}{\partial w_{l-2}} \\
&D_{b_{l-2}}=\frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial z_{l-2}} \dot\
\frac{\partial z_{l-2}}{\partial b_{l-2}}
\end{aligned}
$$
我们可以提取出公式中递归部分
$$
\frac{\partial L}{\partial z_{l}} \quad \frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \quad \frac{\partial L}{\partial z_{l}} \dot\ \frac{\partial z_{l}}{\partial z_{l-1}} \dot\ \frac{\partial z_{l-1}}{\partial z_{l-2}}
$$
则设
$$
\begin{aligned}
\delta_{k}&=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \\
&=\delta_{k+1} \dot\ \frac{\partial z_{k+1}}{\partial z_{k}}
\end{aligned}
$$
所以对于每一层只需要计算
$$
\frac{\partial z_{k+1}}{\partial z_{k}} \quad \frac{\partial z_{k}}{\partial w_{k}} \quad \frac{\partial z_{k}}{\partial b_{k}}
$$
则逐层递归就可以求出每一层权重梯度了，再通过梯度下降公式更新权重



#### 计算

我们现在来计算
$$
\frac{\partial z_{k+1}}{\partial z_{k}} \quad \frac{\partial z_{k}}{\partial w_{k}} \quad \frac{\partial z_{k}}{\partial b_{k}}
$$
由隐层计算公式可知
$$
\begin{aligned}
\frac{\partial z_{k+1}}{\partial z_{k}}&=
\frac{\partial (w_{k+1}a_{k}+b_{k+1})}{\partial z_{k}} \\
&=\frac{\partial (w_{k+1} \sigma(z_{k})+b_{k+1})}{\partial z_{k}} \\
&=(w_{k+1})^{T} \odot \sigma^{'}(z_{k})
\end{aligned}
$$
则
$$
\begin{aligned}
\delta_{k}&=(w_{k+1})^{T} \delta_{k+1} \odot \sigma^{'}(z_{k})
\end{aligned}
$$
再计算权重的偏导
$$
\begin{aligned}
&\frac{\partial z_{k}}{\partial w_{k}}
=\frac{\partial (w_{k}a_{k-1}+b_{k})}{\partial w_{k}}=(a_{k-1})^{T} \\
&\frac{\partial z_{k}}{\partial b_{k}}
=\frac{\partial (w_{k}a_{k-1}+b_{k})}{\partial b_{k}}=1
\end{aligned}
$$
所以综上所述
$$
\begin{aligned}
&D_{w_{k}}=\delta_{k} (a_{k-1})^{T} \\
&D_{b_{k}}=\delta_{k}
\end{aligned}
$$



#### 实现

在代码实现时，有一些细节需要注意

以上推到过程中，我们总是计算Zk（未激活计算结果）的偏导，如果这样，在代码实现时，相关数据信息需要从上一层获取，这样的设计显然不合理

既然神经网络是一个复合函数，那么链式求导公式（如下），是不是也可以换一种写法
$$
\begin{aligned}
&D_{w_{k}}=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \dot\ \frac{\partial z_{k}}{\partial w_{k}} \\
&D_{b_{k}}=\frac{\partial L}{\partial z_{L}} \dot\ \frac{\partial z_{L}}{\partial z_{L-1}} \dot\
\frac{\partial z_{L-1}}{\partial z_{L-2}} \cdots\ \frac{\partial z_{k+1}}{\partial z_{k}} \dot\ \frac{\partial z_{k}}{\partial b_{k}}
\end{aligned}
$$
新写法如下
$$
\begin{aligned}
&D_{w_{k}}=\frac{\partial L}{\partial a_{L}} \dot\ \frac{\partial a_{L}}{\partial a_{L-1}} \dot\
\frac{\partial a_{L-1}}{\partial a_{L-2}} \cdots\ \frac{\partial a_{k+1}}{\partial a_{k}} \dot\ \frac{\partial a_{k}}{\partial w_{k}} \\
&D_{b_{k}}=\frac{\partial L}{\partial a_{L}} \dot\ \frac{\partial a_{L}}{\partial a_{L-1}} \dot\
\frac{\partial a_{L-1}}{\partial a_{L-2}} \cdots\ \frac{\partial a_{k+1}}{\partial a_{k}} \dot\ \frac{\partial a_{k}}{\partial b_{k}} \\
&a_{k}=\sigma(z_{k})
\end{aligned}
$$

这样做的意义在于，每一层计算所需的数据全部由本层数据决定，这样就做到了每一层计算的独立

因为上一层的输入等于下一层的输出，设每一层的输入为d，则
$$
d_{k}=a_{k-1}
$$
那么其他推导公式做如下调整
$$
\begin{aligned}
\frac{\partial a_{k}}{\partial a_{k-1}}
&=\frac{\partial \sigma(w_{k}a_{k-1}+b_{k})}{\partial a_{k-1}} \\
&=(w_{k})^{T} \odot\ \sigma^{'}(w_{k}a_{k-1}+b_{k}) \\
&= (w_{k})^{T} \odot\  \sigma^{'}(a_{k})
\end{aligned}
$$
则
$$
\begin{aligned}
\delta_{k}&=(w_{k})^{T} \delta_{k+1} \odot \sigma^{'}(a_{k})
\end{aligned}
$$
再计算权重的偏导
$$
\begin{aligned}
&\frac{\partial a_{k}}{\partial w_{k}}
=\frac{\partial \sigma(w_{k}a_{k-1}+b_{k})}{\partial w_{k}}=(a_{k-1})^{T} \odot\  \sigma^{'}(w_{k}a_{k-1}+b_{k})
= (d_{k})^{T} \odot\ \sigma^{'}(a_{k})\\
&\frac{\partial a_{k}}{\partial b_{k}}
=\frac{\partial \sigma(w_{k}a_{k-1}+b_{k})}{\partial b_{k}}=\sigma^{'}(w_{k}a_{k-1}+b_{k})=\sigma^{'}(a_{k})
\end{aligned}
$$

所以综上所述
$$
\begin{aligned}
&D_{w_{k}}= (\delta_{k-1} \odot\ \sigma^{'}(a_{k}))d_{k} \\
&D_{b_{k}}=\delta_{k-1} \odot \sigma^{'}(a_{k})
\end{aligned}
$$


### CNN

卷积神经网络的基本结构由卷积、池化、全连接组成，虽然现在还有残差、直连、批次归一化等操作，但是基本结构是不变的

#### 卷积Convolution

卷积神经网络最基本操作就是卷积，如下图，卷积核以滑动窗口的模式，分别与对应元素相乘并求和

![](.\image\卷积.png)
$$
\begin{aligned}
y_{11}=x_{11}w_{11}+x_{12}w_{12}+x_{13}w_{13}+x_{21}w_{21}+x_{22}&w_{22}+x_{23}w_{23}+x_{31}w_{31}+x_{32}w_{32}+x_{33}w_{33} \\
y_{12}=x_{12}w_{11}+x_{13}w_{12}+x_{14}w_{13}+x_{22}w_{21}+x_{23}&w_{22}+x_{24}w_{23}+x_{32}w_{31}+x_{33}w_{32}+x_{34}w_{33} \\
&\vdots
\end{aligned}
$$


#### 池化Pooling

池化一般用于降采样过程，在很多前沿的网络结构中以经不再使用该处理过程，转而直接使用纯卷积网络

![](.\image\池化.png)

##### 均值池化

取区域内元素的平均值
$$
\begin{aligned}
y_{11}=\frac{x_{11}+x_{12}+x_{21}+x_{22}}{4} \\
y_{12}=\frac{x_{13}+x_{14}+x_{23}+x_{24}}{4} \\
y_{21}=\frac{x_{31}+x_{32}+x_{41}+x_{42}}{4} \\
y_{22}=\frac{x_{33}+x_{34}+x_{43}+x_{44}}{4}
\end{aligned}
$$


##### 最大值池化

取区域内元素的最大值
$$
\begin{aligned}
y_{11}=max(x_{11},x_{12},x_{21},x_{22}) \\
y_{12}=max(x_{13},x_{14},x_{23},x_{24}) \\
y_{21}=max(x_{31},x_{32},x_{41},x_{42}) \\
y_{22}=max(x_{33},x_{34},x_{43},x_{44})
\end{aligned}
$$

#### 卷积计算

卷积神经网络最主要的计算消耗就是卷积计算

##### im2col

图像的卷积计算有很多种算法，目前各个开源框架常用的都是im2col+gemm的方式

该方法的做法，就是将卷积过程转化为矩阵乘法，其好处在于可以通过优化矩阵乘算法，优化计算过程，并且有利于CUDA等并行计算

设有图像A，它的一个通道数据如下：

![](.\image\im2col.png)

卷积核W，展开为列向量如下：

![](.\image\kernel.png)

我们将每次卷积计算时，卷积核覆盖的元素分别列出如下：

![](.\image\展开.png)

将这些展开的元素行向量组合为矩阵如下：

![](.\image\im2col展开.drawio.png)

则卷积过程可表示为如下矩阵乘：

![](.\image\卷积过程.drawio.png)

则每一次卷积运算可表示如下：
$$
\begin{aligned}
&z_{l}=w_{l}a_{l-1} \\
&a_{l}=\sigma(z_{l})=\sigma(w_{l}a_{l-1})
\end{aligned}
$$

**图像的多通道**

卷积层的图像都是三维张量n\*m\*z，对于多通道的图像使用一个卷积核卷积，每一个通道都会生成一份结果，我们需要将它们累加起来

设有一个通道为2的图像，我们的卷积过程如下

通道1：

![](C:\Users\yuzhu\Desktop\docs\image\channel1.drawio.png)

通道2：

![](.\image\channel2.drawio.png)



最终卷积结果应该为：

![](C:\Users\yuzhu\Desktop\docs\image\结果加.drawio.png)



在代码实现中以Caffe和darknet为例，上述数据在经过im2col处理后应当如下

![](.\image\卷积乘.drawio.png)

一下是darknet中im2col的代码 

```C
void im2col_cpu(float* data_im,
     int channels,  int height,  int width,
     int ksize,  int stride, int pad, float* data_col)
{
    int c,h,w;
    int height_col = (height + 2*pad - ksize) / stride + 1;
    int width_col = (width + 2*pad - ksize) / stride + 1;
    int channels_col = channels * ksize * ksize;
    for (c = 0; c < channels_col; ++c) {
        int w_offset = c % ksize;
        int h_offset = (c / ksize) % ksize;
        int c_im = c / ksize / ksize;
        for (h = 0; h < height_col; ++h) {
            for (w = 0; w < width_col; ++w) {
                int im_row = h_offset + h * stride;
                int im_col = w_offset + w * stride;
                int col_index = (c * height_col + h) * width_col + w;
                data_col[col_index] = im2col_get_pixel(data_im, height, width, channels,
                        im_row, im_col, c_im, pad);
            }
        }
    }
}
```



##### 多卷积核

一般一个卷积层有多个卷积核filters

设有两个卷积核W1、W2

那么卷积过程如下：

![](.\image\多通道卷积.drawio.png)



##### Bias偏移量

和全连接神经网络一样，需要加上偏移量
$$
w_{k}a_{k-1}+b_{k}
$$
在卷积神经网络中，有些许不同，不是每一个输出值都对应一个偏移值，而是一个channel对应一个相同的偏移值

以上述示例为例，两个卷积核，结果为两个通道

![](.\image\add_bias.drawio.png)

所以卷积层bias数量和卷积核数量filters一样



##### 激活函数

计算结果的每一个值都需要通过激活函数计算



#### 权重更新

综上所述，将卷积过程表示为了矩阵乘，那么直接套用全连接神经网络的推导过程，完全相同的实现过程

在卷积神经网络中，唯一需要注意的是池化层的处理

![](.\image\池化.png)

##### 均值池化

均值池化的计算可以表示如下

![](.\image\池化.drawio.png)
$$
a_{k}=wa_{k-1}
$$

$$
\begin{aligned}
\frac{\partial a_{k+1}}{\partial a_{k}}&=\frac{\partial wa_{k}}{\partial a_{k}}=w^{T}
\end{aligned}
$$

### 损失函数

softmax



cross entropy（交叉熵损失）
